{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23b81774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a654a",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd396f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation features: (414, 118)\n",
      "Reactivity features: (414, 63)\n",
      "Number of subjects: 23\n",
      "Number of videos: 18\n",
      "Trials per subject: 18.0\n"
     ]
    }
   ],
   "source": [
    "INPUT_CONCAT = \"../data/processed/eeg_features_concat.csv\"\n",
    "INPUT_REACT = \"../data/processed/eeg_features_reactivity.csv\"\n",
    "\n",
    "df_concat = pd.read_csv(INPUT_CONCAT)\n",
    "df_react = pd.read_csv(INPUT_REACT)\n",
    "\n",
    "print(f\"Concatenation features: {df_concat.shape}\")\n",
    "print(f\"Reactivity features: {df_react.shape}\")\n",
    "print(f\"Number of subjects: {df_concat['subject_id'].nunique()}\")\n",
    "print(f\"Number of videos: {df_concat['video_id'].nunique()}\")\n",
    "print(f\"Trials per subject: {len(df_concat) / df_concat['subject_id'].nunique():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea8451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_subject_identification_DDM(df, approach_name, n_features=20):\n",
    "    \"\"\"\n",
    "    Distance Discrimination Method (DDM) for subject identification.\n",
    "    \n",
    "    METHOD:\n",
    "    For each trial, calculate:\n",
    "    - Distance to all other trials from SAME subject (within-subject)\n",
    "    - Distance to all trials from DIFFERENT subjects (between-subject)\n",
    "    - Classification: Is this trial's nearest neighbor from the same subject?\n",
    "    \n",
    "    CONTROL: Also check if nearest neighbor is from same video (content-driven)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Feature dataframe with 'subject_id', 'video_id', etc.\n",
    "    approach_name : str\n",
    "        Name of the approach (e.g., 'Concatenation', 'Reactivity')\n",
    "    n_features : int\n",
    "        Number of top features to select (reduces overfitting)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"   {approach_name.upper()} APPROACH - DDM\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    metadata_cols = ['subject_id', 'video_id', 'valence', 'arousal']\n",
    "    X = df.drop(columns=metadata_cols, errors='ignore')\n",
    "    y_subject = df['subject_id'].values\n",
    "    y_video = df['video_id'].values\n",
    "    \n",
    "    print(f\"Features: {X.shape[1]}\")\n",
    "    print(f\"Subjects: {len(np.unique(y_subject))}\")\n",
    "    print(f\"Videos: {len(np.unique(y_video))}\")\n",
    "    print(f\"Total trials: {len(df)}\")\n",
    "    \n",
    "    # Feature selection (reduce overfitting)\n",
    "    # For DDM: Select features with highest variance (most informative for Euclidean distances)\n",
    "    print(f\"\\nSelecting top {n_features} features by variance...\")\n",
    "    feature_variances = np.var(X, axis=0)\n",
    "    top_k_indices = np.argsort(feature_variances)[-min(n_features, X.shape[1]):]\n",
    "    X_selected = X.iloc[:, top_k_indices].values\n",
    "    selected_features = X.columns[top_k_indices].tolist()\n",
    "    print(f\"Selected features (by variance): {selected_features[:5]}...\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "    \n",
    "    print(\"\\nRunning Distance Discrimination Method...\")\n",
    "    print(\"For each trial: Is the nearest neighbor from the same subject/video?\")\n",
    "    \n",
    "    from sklearn.metrics import pairwise_distances\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.model_selection import LeaveOneOut\n",
    "    \n",
    "    # Method 1: Leave-One-Out k-NN (k=1) for SUBJECT identification\n",
    "    print(\"\\n[Method 1] Leave-One-Out Nearest Neighbor Classification (SUBJECT)\")\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    correct_subject = 0\n",
    "    correct_video = 0\n",
    "    total = 0\n",
    "    \n",
    "    for train_idx, test_idx in loo.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_subject[train_idx], y_subject[test_idx]\n",
    "        \n",
    "        knn.fit(X_train, y_train)\n",
    "        pred = knn.predict(X_test)\n",
    "        \n",
    "        if pred[0] == y_test[0]:\n",
    "            correct_subject += 1\n",
    "        total += 1\n",
    "    \n",
    "    loo_accuracy = correct_subject / total\n",
    "    chance_level_subject = 1.0 / len(np.unique(y_subject))\n",
    "    chance_level_video = 1.0 / len(np.unique(y_video))\n",
    "    \n",
    "    # Method 2: Detailed DDM with distance analysis\n",
    "    print(\"\\n[Method 2] Detailed Distance Discrimination Analysis\")\n",
    "    \n",
    "    distance_matrix = pairwise_distances(X_scaled, metric='euclidean')\n",
    "    \n",
    "    within_subject_dists = []\n",
    "    between_subject_dists = []\n",
    "    correct_subject_identifications = []\n",
    "    correct_video_identifications = []\n",
    "    same_subject_same_video = 0\n",
    "    same_subject_diff_video = 0\n",
    "    diff_subject_same_video = 0\n",
    "    diff_subject_diff_video = 0\n",
    "    \n",
    "    for i in range(len(y_subject)):\n",
    "        # Get distances from trial i to all other trials\n",
    "        dists_from_i = distance_matrix[i]\n",
    "        \n",
    "        # Separate into same-subject and different-subject distances\n",
    "        same_subject_mask = (y_subject == y_subject[i]) & (np.arange(len(y_subject)) != i)\n",
    "        diff_subject_mask = (y_subject != y_subject[i])\n",
    "        \n",
    "        same_subject_dists = dists_from_i[same_subject_mask]\n",
    "        diff_subject_dists = dists_from_i[diff_subject_mask]\n",
    "        \n",
    "        if len(same_subject_dists) > 0:\n",
    "            # Store distances for statistics\n",
    "            within_subject_dists.extend(same_subject_dists)\n",
    "            between_subject_dists.extend(diff_subject_dists)\n",
    "            \n",
    "            # Find nearest neighbor (excluding self)\n",
    "            nearest_idx = np.argmin(dists_from_i[dists_from_i > 0])\n",
    "            # Adjust index (since we excluded i=0)\n",
    "            actual_indices = np.where(dists_from_i > 0)[0]\n",
    "            nearest_actual_idx = actual_indices[nearest_idx]\n",
    "            \n",
    "            # Check if nearest neighbor is from same subject\n",
    "            if y_subject[nearest_actual_idx] == y_subject[i]:\n",
    "                correct_subject_identifications.append(1)\n",
    "            else:\n",
    "                correct_subject_identifications.append(0)\n",
    "            \n",
    "            # Check if nearest neighbor is from same video\n",
    "            if y_video[nearest_actual_idx] == y_video[i]:\n",
    "                correct_video_identifications.append(1)\n",
    "            else:\n",
    "                correct_video_identifications.append(0)\n",
    "            \n",
    "            # Categorize the nearest neighbor\n",
    "            same_subj = (y_subject[nearest_actual_idx] == y_subject[i])\n",
    "            same_vid = (y_video[nearest_actual_idx] == y_video[i])\n",
    "            \n",
    "            if same_subj and same_vid:\n",
    "                same_subject_same_video += 1\n",
    "            elif same_subj and not same_vid:\n",
    "                same_subject_diff_video += 1\n",
    "            elif not same_subj and same_vid:\n",
    "                diff_subject_same_video += 1\n",
    "            else:\n",
    "                diff_subject_diff_video += 1\n",
    "    \n",
    "    ddm_accuracy_subject = np.mean(correct_subject_identifications)\n",
    "    ddm_accuracy_video = np.mean(correct_video_identifications)\n",
    "    \n",
    "    # Calculate distance statistics\n",
    "    avg_within = np.mean(within_subject_dists)\n",
    "    std_within = np.std(within_subject_dists)\n",
    "    avg_between = np.mean(between_subject_dists)\n",
    "    std_between = np.std(between_subject_dists)\n",
    "    \n",
    "    # Separation metrics\n",
    "    separation_ratio = avg_between / avg_within if avg_within > 0 else 0\n",
    "    cohen_d = (avg_between - avg_within) / np.sqrt((std_within**2 + std_between**2) / 2)\n",
    "    \n",
    "    # Statistical tests\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Test 1: Are within and between distances significantly different?\n",
    "    t_stat, p_value_dist = stats.ttest_ind(within_subject_dists, \n",
    "                                            np.random.choice(between_subject_dists, \n",
    "                                                           size=min(len(within_subject_dists), \n",
    "                                                                   len(between_subject_dists))))\n",
    "    \n",
    "    # Test 2: Is subject accuracy significantly above chance?\n",
    "    from scipy.stats import binomtest\n",
    "    n_trials = len(correct_subject_identifications)\n",
    "    n_successes_subject = sum(correct_subject_identifications)\n",
    "    binom_result_subject = binomtest(n_successes_subject, n_trials, chance_level_subject, alternative='greater')\n",
    "    p_value_acc_subject = binom_result_subject.pvalue\n",
    "    \n",
    "    # Test 3: Is video accuracy significantly above chance?\n",
    "    n_successes_video = sum(correct_video_identifications)\n",
    "    binom_result_video = binomtest(n_successes_video, n_trials, chance_level_video, alternative='greater')\n",
    "    p_value_acc_video = binom_result_video.pvalue\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nüìä SUBJECT IDENTIFICATION (Primary Goal):\")\n",
    "    print(f\"  LOO k-NN Accuracy:    {loo_accuracy*100:.2f}%\")\n",
    "    print(f\"  DDM Accuracy:         {ddm_accuracy_subject*100:.2f}%\")\n",
    "    print(f\"  Chance Level:         {chance_level_subject*100:.2f}%\")\n",
    "    print(f\"  Above Chance:         {(ddm_accuracy_subject - chance_level_subject)*100:.2f} percentage points\")\n",
    "    print(f\"  Improvement Factor:   {ddm_accuracy_subject / chance_level_subject:.2f}x\")\n",
    "    \n",
    "    print(f\"\\nüé¨ VIDEO IDENTIFICATION (Control Check):\")\n",
    "    print(f\"  DDM Accuracy:         {ddm_accuracy_video*100:.2f}%\")\n",
    "    print(f\"  Chance Level:         {chance_level_video*100:.2f}%\")\n",
    "    print(f\"  Above Chance:         {(ddm_accuracy_video - chance_level_video)*100:.2f} percentage points\")\n",
    "    print(f\"  Improvement Factor:   {ddm_accuracy_video / chance_level_video:.2f}x\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"\\nüîç NEAREST NEIGHBOR BREAKDOWN:\")\n",
    "    print(f\"  Same Subject, Same Video:   {same_subject_same_video:3d} ({same_subject_same_video/n_trials*100:.1f}%)\")\n",
    "    print(f\"  Same Subject, Diff Video:   {same_subject_diff_video:3d} ({same_subject_diff_video/n_trials*100:.1f}%) ‚úÖ IDEAL\")\n",
    "    print(f\"  Diff Subject, Same Video:   {diff_subject_same_video:3d} ({diff_subject_same_video/n_trials*100:.1f}%) ‚ö†Ô∏è  CONFOUND\")\n",
    "    print(f\"  Diff Subject, Diff Video:   {diff_subject_diff_video:3d} ({diff_subject_diff_video/n_trials*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüí° INTERPRETATION:\")\n",
    "    if ddm_accuracy_subject > ddm_accuracy_video:\n",
    "        print(f\"  ‚úÖ PERSON-DRIVEN: Features capture individual identity > video content\")\n",
    "    elif ddm_accuracy_video > ddm_accuracy_subject:\n",
    "        print(f\"  ‚ö†Ô∏è  CONTENT-DRIVEN: Features capture video content > individual identity\")\n",
    "    else:\n",
    "        print(f\"  ‚öñÔ∏è  MIXED: Similar influence from person and video\")\n",
    "    \n",
    "    print(f\"\\nüìè DISTANCE ANALYSIS:\")\n",
    "    print(f\"  Within-subject:   {avg_within:.4f} ¬± {std_within:.4f}\")\n",
    "    print(f\"  Between-subject:  {avg_between:.4f} ¬± {std_between:.4f}\")\n",
    "    print(f\"  Separation Ratio: {separation_ratio:.4f}\")\n",
    "    print(f\"  Cohen's d:        {cohen_d:.4f}\")\n",
    "    \n",
    "    if separation_ratio > 1.2:\n",
    "        print(f\"  ‚úÖ STRONG separability (ratio > 1.2)\")\n",
    "    elif separation_ratio > 1.1:\n",
    "        print(f\"  ‚úÖ Good separability (ratio > 1.1)\")\n",
    "    elif separation_ratio > 1.0:\n",
    "        print(f\"  ‚ö†Ô∏è  Weak separability (ratio > 1.0)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå NO separability (ratio ‚â§ 1.0)\")\n",
    "    \n",
    "    print(f\"\\nüìà STATISTICAL SIGNIFICANCE:\")\n",
    "    print(f\"  Distance test (t-test):\")\n",
    "    print(f\"    t = {t_stat:.3f}, p = {p_value_dist:.4f}\")\n",
    "    if p_value_dist < 0.001:\n",
    "        print(f\"    ‚úÖ Highly significant (p < 0.001)\")\n",
    "    elif p_value_dist < 0.05:\n",
    "        print(f\"    ‚úÖ Significant (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"    ‚ùå Not significant (p ‚â• 0.05)\")\n",
    "    \n",
    "    print(f\"\\n  Subject accuracy test (binomial):\")\n",
    "    print(f\"    {n_successes_subject}/{n_trials} correct, p = {p_value_acc_subject:.4f}\")\n",
    "    if p_value_acc_subject < 0.001:\n",
    "        print(f\"    ‚úÖ Highly significant (p < 0.001)\")\n",
    "    elif p_value_acc_subject < 0.05:\n",
    "        print(f\"    ‚úÖ Significant (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"    ‚ùå Not significant (p ‚â• 0.05)\")\n",
    "    \n",
    "    print(f\"\\n  Video accuracy test (binomial):\")\n",
    "    print(f\"    {n_successes_video}/{n_trials} correct, p = {p_value_acc_video:.4f}\")\n",
    "    if p_value_acc_video < 0.001:\n",
    "        print(f\"    ‚ö†Ô∏è  Highly significant (p < 0.001) - Video confound!\")\n",
    "    elif p_value_acc_video < 0.05:\n",
    "        print(f\"    ‚ö†Ô∏è  Significant (p < 0.05) - Video confound!\")\n",
    "    else:\n",
    "        print(f\"    ‚úÖ Not significant - Good! No video confound\")\n",
    "    \n",
    "    return {\n",
    "        'approach': approach_name,\n",
    "        'loo_accuracy': loo_accuracy,\n",
    "        'ddm_accuracy_subject': ddm_accuracy_subject,\n",
    "        'ddm_accuracy_video': ddm_accuracy_video,\n",
    "        'chance_level_subject': chance_level_subject,\n",
    "        'chance_level_video': chance_level_video,\n",
    "        'within_dist_mean': avg_within,\n",
    "        'within_dist_std': std_within,\n",
    "        'between_dist_mean': avg_between,\n",
    "        'between_dist_std': std_between,\n",
    "        'separation_ratio': separation_ratio,\n",
    "        'cohen_d': cohen_d,\n",
    "        'n_features': X_selected.shape[1],\n",
    "        'selected_features': selected_features,\n",
    "        'p_value_distance': p_value_dist,\n",
    "        'p_value_acc_subject': p_value_acc_subject,\n",
    "        'p_value_acc_video': p_value_acc_video,\n",
    "        'same_subject_same_video': same_subject_same_video,\n",
    "        'same_subject_diff_video': same_subject_diff_video,\n",
    "        'diff_subject_same_video': diff_subject_same_video,\n",
    "        'diff_subject_diff_video': diff_subject_diff_video\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1d43d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "   CONCATENATION APPROACH - DDM\n",
      "============================================================\n",
      "Features: 114\n",
      "Subjects: 23\n",
      "Videos: 18\n",
      "Total trials: 414\n",
      "\n",
      "Selecting top 20 features...\n",
      "Selected features: ['Stim_RMS_AF3', 'Stim_RMS_F7', 'Stim_RMS_F3', 'Stim_RMS_FC5', 'Base_Beta_AF3']...\n",
      "\n",
      "Running Distance Discrimination Method...\n",
      "For each trial: Is the nearest neighbor from the same subject/video?\n",
      "\n",
      "[Method 1] Leave-One-Out Nearest Neighbor Classification (SUBJECT)\n",
      "\n",
      "[Method 2] Detailed Distance Discrimination Analysis\n",
      "\n",
      "============================================================\n",
      "RESULTS:\n",
      "============================================================\n",
      "\n",
      "üìä SUBJECT IDENTIFICATION (Primary Goal):\n",
      "  LOO k-NN Accuracy:    51.69%\n",
      "  DDM Accuracy:         51.69%\n",
      "  Chance Level:         4.35%\n",
      "  Above Chance:         47.34 percentage points\n",
      "  Improvement Factor:   11.89x\n",
      "\n",
      "üé¨ VIDEO IDENTIFICATION (Control Check):\n",
      "  DDM Accuracy:         2.66%\n",
      "  Chance Level:         5.56%\n",
      "  Above Chance:         -2.90 percentage points\n",
      "  Improvement Factor:   0.48x\n",
      "\n",
      "üîç NEAREST NEIGHBOR BREAKDOWN:\n",
      "  Same Subject, Same Video:     0 (0.0%)\n",
      "  Same Subject, Diff Video:   214 (51.7%) ‚úÖ IDEAL\n",
      "  Diff Subject, Same Video:    11 (2.7%) ‚ö†Ô∏è  CONFOUND\n",
      "  Diff Subject, Diff Video:   189 (45.7%)\n",
      "\n",
      "üí° INTERPRETATION:\n",
      "  ‚úÖ PERSON-DRIVEN: Features capture individual identity > video content\n",
      "\n",
      "üìè DISTANCE ANALYSIS:\n",
      "  Within-subject:   1.9274 ¬± 4.3954\n",
      "  Between-subject:  3.1912 ¬± 5.5358\n",
      "  Separation Ratio: 1.6557\n",
      "  Cohen's d:        0.2529\n",
      "  ‚úÖ STRONG separability (ratio > 1.2)\n",
      "\n",
      "üìà STATISTICAL SIGNIFICANCE:\n",
      "  Distance test (t-test):\n",
      "    t = -14.775, p = 0.0000\n",
      "    ‚úÖ Highly significant (p < 0.001)\n",
      "\n",
      "  Subject accuracy test (binomial):\n",
      "    214/414 correct, p = 0.0000\n",
      "    ‚úÖ Highly significant (p < 0.001)\n",
      "\n",
      "  Video accuracy test (binomial):\n",
      "    11/414 correct, p = 0.9984\n",
      "    ‚úÖ Not significant - Good! No video confound\n"
     ]
    }
   ],
   "source": [
    "results_concat = evaluate_subject_identification_DDM(df_concat, \"Concatenation\", n_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6b41382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "   REACTIVITY APPROACH - DDM\n",
      "============================================================\n",
      "Features: 59\n",
      "Subjects: 23\n",
      "Videos: 18\n",
      "Total trials: 414\n",
      "\n",
      "Selecting top 20 features...\n",
      "Selected features: ['ŒîFAA', 'ŒîCorr_F3F4', 'ŒîCorr_F7F8', 'ŒîRMS_AF3', 'ŒîRMS_F7']...\n",
      "\n",
      "Running Distance Discrimination Method...\n",
      "For each trial: Is the nearest neighbor from the same subject/video?\n",
      "\n",
      "[Method 1] Leave-One-Out Nearest Neighbor Classification (SUBJECT)\n",
      "\n",
      "[Method 2] Detailed Distance Discrimination Analysis\n",
      "\n",
      "============================================================\n",
      "RESULTS:\n",
      "============================================================\n",
      "\n",
      "üìä SUBJECT IDENTIFICATION (Primary Goal):\n",
      "  LOO k-NN Accuracy:    20.53%\n",
      "  DDM Accuracy:         20.53%\n",
      "  Chance Level:         4.35%\n",
      "  Above Chance:         16.18 percentage points\n",
      "  Improvement Factor:   4.72x\n",
      "\n",
      "üé¨ VIDEO IDENTIFICATION (Control Check):\n",
      "  DDM Accuracy:         5.07%\n",
      "  Chance Level:         5.56%\n",
      "  Above Chance:         -0.48 percentage points\n",
      "  Improvement Factor:   0.91x\n",
      "\n",
      "üîç NEAREST NEIGHBOR BREAKDOWN:\n",
      "  Same Subject, Same Video:     0 (0.0%)\n",
      "  Same Subject, Diff Video:    85 (20.5%) ‚úÖ IDEAL\n",
      "  Diff Subject, Same Video:    21 (5.1%) ‚ö†Ô∏è  CONFOUND\n",
      "  Diff Subject, Diff Video:   308 (74.4%)\n",
      "\n",
      "üí° INTERPRETATION:\n",
      "  ‚úÖ PERSON-DRIVEN: Features capture individual identity > video content\n",
      "\n",
      "üìè DISTANCE ANALYSIS:\n",
      "  Within-subject:   3.4601 ¬± 5.1092\n",
      "  Between-subject:  3.6683 ¬± 5.1698\n",
      "  Separation Ratio: 1.0602\n",
      "  Cohen's d:        0.0405\n",
      "  ‚ö†Ô∏è  Weak separability (ratio > 1.0)\n",
      "\n",
      "üìà STATISTICAL SIGNIFICANCE:\n",
      "  Distance test (t-test):\n",
      "    t = -2.315, p = 0.0206\n",
      "    ‚úÖ Significant (p < 0.05)\n",
      "\n",
      "  Subject accuracy test (binomial):\n",
      "    85/414 correct, p = 0.0000\n",
      "    ‚úÖ Highly significant (p < 0.001)\n",
      "\n",
      "  Video accuracy test (binomial):\n",
      "    21/414 correct, p = 0.6961\n",
      "    ‚úÖ Not significant - Good! No video confound\n"
     ]
    }
   ],
   "source": [
    "results_react = evaluate_subject_identification_DDM(df_react, \"Reactivity\", n_features=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d65636",
   "metadata": {},
   "source": [
    "## 3. Save Results & Compare Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fb2da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved DDM results to: ../outputs/models/ml_comparison_DDM.csv\n",
      "\n",
      "============================================================\n",
      "DDM ANALYSIS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìã SUMMARY:\n",
      "  Concatenation: 51.69% subject accuracy\n",
      "  Reactivity:    20.53% subject accuracy\n",
      "  Difference:    31.16 percentage points\n",
      "\n",
      "  Video confound: ‚úÖ NONE (both approaches)\n"
     ]
    }
   ],
   "source": [
    "# Save ML results\n",
    "ml_results_df = pd.DataFrame([\n",
    "    {\n",
    "        'model': f\"DDM - {results_concat['approach']}\",\n",
    "        'subject_accuracy': results_concat['ddm_accuracy_subject'],\n",
    "        'video_accuracy': results_concat['ddm_accuracy_video'],\n",
    "        'loo_accuracy': results_concat['loo_accuracy'],\n",
    "        'chance_level_subject': results_concat['chance_level_subject'],\n",
    "        'chance_level_video': results_concat['chance_level_video'],\n",
    "        'separation_ratio': results_concat['separation_ratio'],\n",
    "        'cohen_d': results_concat['cohen_d'],\n",
    "        'n_features': results_concat['n_features'],\n",
    "        'p_value_subject': results_concat['p_value_acc_subject'],\n",
    "        'p_value_video': results_concat['p_value_acc_video'],\n",
    "        'p_value_distance': results_concat['p_value_distance'],\n",
    "        'same_subj_same_vid': results_concat['same_subject_same_video'],\n",
    "        'same_subj_diff_vid': results_concat['same_subject_diff_video'],\n",
    "        'diff_subj_same_vid': results_concat['diff_subject_same_video'],\n",
    "        'diff_subj_diff_vid': results_concat['diff_subject_diff_video']\n",
    "    },\n",
    "    {\n",
    "        'model': f\"DDM - {results_react['approach']}\",\n",
    "        'subject_accuracy': results_react['ddm_accuracy_subject'],\n",
    "        'video_accuracy': results_react['ddm_accuracy_video'],\n",
    "        'loo_accuracy': results_react['loo_accuracy'],\n",
    "        'chance_level_subject': results_react['chance_level_subject'],\n",
    "        'chance_level_video': results_react['chance_level_video'],\n",
    "        'separation_ratio': results_react['separation_ratio'],\n",
    "        'cohen_d': results_react['cohen_d'],\n",
    "        'n_features': results_react['n_features'],\n",
    "        'p_value_subject': results_react['p_value_acc_subject'],\n",
    "        'p_value_video': results_react['p_value_acc_video'],\n",
    "        'p_value_distance': results_react['p_value_distance'],\n",
    "        'same_subj_same_vid': results_react['same_subject_same_video'],\n",
    "        'same_subj_diff_vid': results_react['same_subject_diff_video'],\n",
    "        'diff_subj_same_vid': results_react['diff_subject_same_video'],\n",
    "        'diff_subj_diff_vid': results_react['diff_subject_diff_video']\n",
    "    }\n",
    "])\n",
    "\n",
    "ml_results_df.to_csv('../outputs/models/ml_comparison_DDM.csv', index=False)\n",
    "print(\"\\n‚úÖ Saved DDM results to: ../outputs/models/ml_comparison_DDM.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DDM ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìã SUMMARY:\")\n",
    "print(f\"  Concatenation: {results_concat['ddm_accuracy_subject']*100:.2f}% subject accuracy\")\n",
    "print(f\"  Reactivity:    {results_react['ddm_accuracy_subject']*100:.2f}% subject accuracy\")\n",
    "print(f\"  Difference:    {(results_concat['ddm_accuracy_subject'] - results_react['ddm_accuracy_subject'])*100:.2f} percentage points\")\n",
    "print(f\"\\n  Video confound: {'‚úÖ NONE (both approaches)' if results_concat['p_value_acc_video'] > 0.05 and results_react['p_value_acc_video'] > 0.05 else '‚ö†Ô∏è  DETECTED'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
