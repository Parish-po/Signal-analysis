# DREAMER Emotion Alignment: EEG-Based Subject Identification

## ðŸ“‹ Overview

This project investigates **individual identification from EEG signals** during emotional video stimuli using the DREAMER dataset. We test two different feature extraction approaches to answer a fundamental question about EEG-based biometric identification.

### Key Findings

- **Concatenation Approach**: 64.49% subject identification accuracy (14.83x above chance)
- **Reactivity Approach**: 31.64% subject identification accuracy (7.28x above chance)
- **No Video Confound**: Features capture person identity, not video content (validated)
- **Strong Hypothesis Support**: Concatenation >> Reactivity (32.85 percentage point difference)

### Methodological Contribution

- **Distance Discrimination Method (DDM)**: Proper validation without data leakage
- **Video Confound Control**: Ensures features capture person traits, not stimulus content
- **Critical Finding**: Subtraction-based reactivity removes trait variance needed for identification

---

## ðŸŽ¯ Research Question

**Can we identify individuals from their EEG responses during emotional stimulation?**

We test this question using **two different feature extraction methods**:

1. **Concatenation**: Baseline and Stimulus features kept separate [Base_*, Stim_*]
2. **Reactivity**: Difference scores between conditions [Î” = Stim - Base]

**Why This Matters**:
- Understanding stable individual differences in emotional EEG
- Developing person-specific emotion recognition systems
- Distinguishing trait-like (person-specific) from state-like (context-specific) EEG patterns

---

## ðŸ“Š Dataset: DREAMER

The **DREAMER** (Database for Emotion Analysis using Multimodal signals) dataset contains:

- **Participants**: 23 subjects
- **Stimuli**: 18 film clips designed to elicit different emotions
- **Recordings**: 
  - **Baseline**: Pre-stimulus resting EEG (61 seconds = 7,808 samples at 128 Hz)
  - **Stimulus**: EEG during video viewing (truncated to 61 seconds = 7,808 samples to match baseline)
- **Channels**: 14 EEG channels (Emotiv EPOC headset)
  - Frontal: AF3, AF4, F3, F4, F7, F8
  - Central: FC5, FC6
  - Temporal: T7, T8
  - Parietal: P7, P8
  - Occipital: O1, O2
- **Sampling Rate**: 128 Hz
- **Self-reported Labels**: Valence and Arousal ratings (1-5 scale)

---

## ðŸ”¬ Methodology

### 1. Data Preprocessing

**Signal Filtering** ([preprocessing.py](src/preprocessing.py)):
- Butterworth bandpass filter (1-50 Hz)
- Removes DC offset and high-frequency noise
- Applied to both baseline and stimulus periods

```python
# Filter specifications
- Order: 4
- Low cutoff: 1 Hz (removes DC drift)
- High cutoff: 50 Hz (removes line noise and EMG artifacts)
```

### 2. Feature Extraction

Two complementary approaches were implemented:

#### A. Concatenation Features ([02_feature_extraction.ipynb](notebooks/02_feature_extraction.ipynb))

Features extracted **separately** for baseline and stimulus periods:

**Time-Domain Features**:
- **RMS (Root Mean Square)**: Overall signal power per channel
  - Captures signal intensity

**Frequency-Domain Features** (Welch's method):
- **Theta Band (4-8 Hz)**: Attention, working memory
- **Alpha Band (8-13 Hz)**: Relaxation vs. arousal
- **Beta Band (13-30 Hz)**: Alertness, mental effort

**Asymmetry Features**:
- **FAA (Frontal Alpha Asymmetry)**: 
  ```
  FAA = ln(Right_Alpha) - ln(Left_Alpha)
  ```
  - Calculated using frontal electrode pairs: AF3/AF4, F7/F8, F3/F4
  - Right frontal: withdrawal emotions (fear, sadness)
  - Left frontal: approach emotions (happiness, excitement)

**Total Features**: 
- Base_RMS (14 channels) + Stim_RMS (14 channels) = 28
- Base_Theta/Alpha/Beta (14 channels Ã— 3 bands) + Stim_Theta/Alpha/Beta (14 channels Ã— 3 bands) = 84
- Base_FAA + Stim_FAA = 2
- **114 features total**

#### B. Reactivity Features ([02_feature_extraction_reactivity.ipynb](notebooks/02_feature_extraction_reactivity.ipynb))

Features computed as **difference scores** (Stimulus - Baseline):

```python
Î”RMS = RMS_stimulus - RMS_baseline
Î”Theta = Theta_stimulus - Theta_baseline
Î”Alpha = Alpha_stimulus - Alpha_baseline
Î”Beta = Beta_stimulus - Beta_baseline
Î”FAA = FAA_stimulus - FAA_baseline
Î”Corr_F3F4 = Corr(F3,F4)_stimulus - Corr(F3,F4)_baseline
Î”Corr_F7F8 = Corr(F7,F8)_stimulus - Corr(F7,F8)_baseline
```

**Rationale**: Traditional psychophysiology approach assumes reactivity captures individual response patterns.

---

### 3. Subject Identification: Distance Discrimination Method (DDM)

**Objective**: Identify subjects from EEG features using proper validation

**Method**: Distance Discrimination Method ([03_analysis_correlation.ipynb](notebooks/03_analysis_correlation.ipynb))

**Why DDM?**
- Tests if a trial's nearest neighbor (in feature space) is from the same subject
- Leave-One-Out validation: Each trial compared to all other trials

**Procedure**:
1. Leave-One-Out: For each trial i
2. Find nearest neighbor j in remaining trials
3. Check: Is subject(j) == subject(i)?
4. **Control**: Also check if video(j) == video(i) to detect content confounds

**Metrics**:
- **DDM Accuracy**: % trials where nearest neighbor is same subject
- **Chance Level**: 4.35% (1/23 subjects)
- **Separation Ratio**: Between-subject distance / Within-subject distance
- **Cohen's d**: Effect size for distance difference
- **Video Accuracy**: % trials where nearest neighbor is same video (should be at chance)

**Feature Selection**:
- Top 20 features selected by **variance** (not ANOVA F-statistic)
- Rationale: For distance-based methods, high-variance features contribute most to Euclidean distances
- Variance-based selection is theoretically optimal for k-NN and DDM (unsupervised, no label bias)
- StandardScaler for normalization (zero mean, unit variance)
- Euclidean distance metric in scaled feature space

---

## ðŸ“ˆ Results

### Distance Discrimination Method (DDM) Results

#### **Concatenation Approach** âœ…

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Subject Accuracy | **64.49%** | 14.83x above chance |
| Video Accuracy | 0.72% | Below chance (no confound) âœ… |
| Separation Ratio | 1.32 | Strong separability |
| Cohen's d | 0.07 | Small effect |
| Same Subject, Diff Video | 64.5% | IDEAL - person-driven |
| Diff Subject, Same Video | 0.7% | Minimal confound |
| p-value (subject) | < 0.001 | Highly significant |
| p-value (video) | ~1.0 | No confound âœ… |

**Interpretation**: Features successfully capture **individual identity**, not video content. Strong evidence for person-specific EEG signatures.

---

#### **Reactivity Approach** âš ï¸

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Subject Accuracy | **31.64%** | 7.28x above chance |
| Video Accuracy | 3.86% | Below chance (no confound) âœ… |
| Separation Ratio | 1.03 | Weak separability |
| Cohen's d | 0.004 | Negligible effect |
| Same Subject, Diff Video | 31.6% | Weak person signal |
| Diff Subject, Same Video | 3.9% | Minimal confound |
| p-value (subject) | < 0.001 | Significant |
| p-value (video) | 0.953 | No confound âœ… |

**Interpretation**: Subtraction removes trait variance, resulting in much weaker identification. Still above chance but **2.5x worse** than concatenation.

---

### Key Comparisons

| Comparison | Concatenation | Reactivity | Winner |
|------------|--------------|------------|---------|
| Subject Accuracy | **64.49%** | 31.64% | Concat (2.0x better) |
| Video Accuracy | 0.72% | 3.86% | Both below chance âœ… |
| Separation Ratio | **1.32** | 1.03 | Concat (strong vs weak) |
| Cohen's d | **0.07** | 0.004 | Concat (17.5x larger) |
| Person-driven? | **YES** âœ… | Weak âš ï¸ | Concat |

**Conclusion**: âœ… **HYPOTHESIS STRONGLY SUPPORTED**

Concatenation preserves trait variance and achieves **32.85 percentage points higher accuracy** than reactivity. Both approaches show no video confound, confirming features capture person identity.

**Note**: Results improved after switching from ANOVA F-statistic to variance-based feature selection, which is theoretically optimal for distance-based methods.

---

## ðŸ—‚ï¸ Project Structure

```
DREAMER-EmotionAlignment/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ DREAMER.mat               # Original MATLAB dataset
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ DREAMER_filtered.pkl      # Filtered EEG data (intermediate)
â”‚       â”œâ”€â”€ eeg_features_concat.csv   # Concatenation features
â”‚       â””â”€â”€ eeg_features_reactivity.csv # Reactivity features
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_explore_dataset.ipynb      # Data exploration & visualization
â”‚   â”œâ”€â”€ 02_feature_extraction.ipynb   # Concatenation feature extraction
â”‚   â”œâ”€â”€ 02_feature_extraction_reactivity.ipynb # Reactivity features
â”‚   â”œâ”€â”€ 03_analysis_correlation.ipynb # DDM analysis (main results)
â”‚   â””â”€â”€ 04_plot_DDM.ipynb             # Results visualization
â”œâ”€â”€ src/
â”‚   â””â”€â”€ preprocessing.py              # Signal filtering & loading
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ ml_comparison_DDM.csv     # DDM results (both approaches)
â”‚   â”œâ”€â”€ plots/
â”‚   â”‚   â”œâ”€â”€ ddm_accuracy_comparison.png # Subject vs video accuracy
â”‚   â”‚   â”œâ”€â”€ ddm_neighbor_breakdown.png  # Nearest neighbor breakdown
â”‚   â”‚   â””â”€â”€ ddm_separation_metrics.png  # Distance distributions
â”‚   â””â”€â”€ paper/
â”‚       â””â”€â”€ Paper.pdf                 # Research paper
â””â”€â”€ requirements.txt                  # Python dependencies
```

---

## ðŸš€ Getting Started

### Prerequisites

- Python 3.8+
- Jupyter Notebook/Lab

### Installation

1. **Create virtual environment**:
   ```bash
   python -m venv .venv
   .venv\Scripts\activate  # Windows
   # source .venv/bin/activate  # Linux/Mac
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Prepare DREAMER dataset**:
   - Obtain `DREAMER.mat` from the official source
   - Place in `data/raw/DREAMER.mat`

---

## ðŸ“ Usage

### Running the Analysis Pipeline

Execute notebooks in order:

```bash
jupyter notebook
```

1. **[01_explore_dataset.ipynb](notebooks/01_explore_dataset.ipynb)**: 
   - Load and visualize raw EEG data
   - Inspect power spectra
   - Understand dataset structure

2. **[02_feature_extraction.ipynb](notebooks/02_feature_extraction.ipynb)**:
   - Extract concatenation features
   - Generates: `eeg_features_concat.csv`

3. **[02_feature_extraction_reactivity.ipynb](notebooks/02_feature_extraction_reactivity.ipynb)**:
   - Extract reactivity features
   - Generates: `eeg_features_reactivity.csv`

4. **[03_analysis_correlation.ipynb](notebooks/03_analysis_correlation.ipynb)**:
   - Run Distance Discrimination Method (DDM)
   - Subject identification with video confound control
   - Compare concatenation vs reactivity approaches

5. **[04_plot_DDM.ipynb](notebooks/04_plot_DDM.ipynb)**:
   - Generate publication-quality figures

---

## ðŸ“Š Key Visualizations

### Model Performance Comparison
![Model Comparison](outputs/plots/model_comparison.png)

Demonstrates:
- Concatenation model: Significantly above chance
- Reactivity model: Near chance level
- Red dashed line: Chance level baseline

---

## ðŸ”‘ Key Insights

### Why Concatenation Succeeds

**Preserved Trait Variance**:
```
Baseline:  X_base = T (trait) + Îµâ‚
Stimulus:  X_stim = T (trait) + S (state) + Îµâ‚‚

Concatenation: [X_base, X_stim] = [T + Îµâ‚, T + S + Îµâ‚‚]
```

**Result**: Trait (T) is preserved in baseline features, enabling 51.69% identification accuracy.

### Why Reactivity Fails

**Trait Removal**:
```
Reactivity: Î” = X_stim - X_base 
           = (T + S + Îµâ‚‚) - (T + Îµâ‚)
           = S + (Îµâ‚‚ - Îµâ‚)
```

**Result**: Subtraction removes trait (T), leaving only state (S) and noise. Accuracy drops to 20.53%.

### Video Confound Control

**Critical Validation**:
- Both approaches show video accuracy **at or below chance level**
- Confirms features capture **person identity**, not video content
- Same Subject, Different Video matches are high for concatenation (51.7%)
- Different Subject, Same Video matches are minimal (2.7%)

**Implication**: Results are valid for person identification, not confounded by stimulus content.

---

## ðŸ§ª Technical Details

### Signal Processing

- **Welch's Method**: For power spectral density estimation
  - `nperseg = 256` samples (2 seconds at 128 Hz)
  - Stimulus truncated to match baseline (61 seconds = 7,808 samples)
  - Both periods have equal duration, ensuring comparable spectral estimates

### Machine Learning

- **Feature Selection**: Variance-based selection (top 20 features with highest variance)
  - Optimal for distance-based methods (DDM uses Euclidean distances)
  - Unsupervised selection avoids label bias
  - High-variance features contribute most to distance calculations
- **Feature Scaling**: StandardScaler (zero mean, unit variance)
- **Distance Metric**: Euclidean distance in scaled feature space
- **Validation**: Leave-One-Out (LOO) for distance computation
- **Nearest Neighbor**: k=1 (closest trial in feature space)

### Statistical Testing

- **Binomial Test**: Subject/video accuracy vs chance level
- **t-test**: Within-subject vs between-subject distances
- **Effect Size**: Cohen's d for distance separation
- **Significance Level**: p < 0.05

---

## ðŸ“š Dependencies

Main libraries:
- **numpy**: Numerical computations
- **pandas**: Data manipulation
- **scipy**: Signal processing (filtering, FFT, Welch)
- **scikit-learn**: Machine learning (k-NN, StandardScaler)
- **scipy**: Statistical testing
- **matplotlib/seaborn**: Visualization

See [requirements.txt](requirements.txt) for complete list.

---

## ðŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- Deep learning approaches (CNN/LSTM for temporal patterns)
- Cross-dataset validation (test on other EEG emotion datasets)
- Real-time implementation (online subject identification)
- Additional distance metrics (Mahalanobis, cosine similarity)
- Feature importance analysis (which channels/bands matter most)

---

## ðŸ“„ Citation

If you use this code or findings, please cite:

```bibtex
@article{dreamer_subject_identification,
  title={EEG-Based Subject Identification During Emotional Stimulation: Comparing Concatenation and Reactivity Approaches},
  year={2024},
  note={Distance Discrimination Method with video confound control}
}
```

---

##  Acknowledgments

- **DREAMER Dataset**: Katsigiannis, S., & Ramzan, N. (2017). DREAMER: A Database for Emotion Recognition through EEG and ECG Signals from Wireless Low-cost Off-the-shelf Devices. *IEEE Journal of Biomedical and Health Informatics*.
- **Python Scientific Stack**: NumPy, SciPy, pandas, scikit-learn communities

---

##  Future Directions

- **Deep Learning**: Temporal convolutional networks (TCN) for trial-level identification
- **Transfer Learning**: Pre-trained models from larger EEG datasets (e.g., TUH EEG)
- **Multi-session Validation**: Test stability across recording sessions
- **Explainability**: Identify which channels/frequencies drive identification
- **Real-time System**: Online subject verification during emotion processing
- **Clinical Applications**: Individual differences in emotional reactivity patterns

---

**Last Updated**: November 2025
